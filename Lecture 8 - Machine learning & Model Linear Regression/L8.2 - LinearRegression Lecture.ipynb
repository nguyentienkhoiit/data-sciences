{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZEb8Tu6ES8YB"},"source":["# Linear Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sHw2X4Vusl97"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","sns.set_style(\"whitegrid\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Install sklearn library:\n","!pip install -U scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uByfyO2nkxIl"},"outputs":[],"source":["# Helper functions\n","\n","def draw_grid(x_lim=np.array([-4, 4]), y_lim=np.array([-4, 4])):\n","    \"\"\"Draw an empty grid\"\"\"\n","    ax = plt.gca()\n","    # Draw ticks and grid\n","    for i in range(int(x_lim.min()), int(x_lim.max())):\n","        ax.axvline(i, linestyle='--', color='#ecf0f1', zorder=0)\n","        ax.plot([i, i], [0.05, -0.05], color='#2c3e50')\n","    for i in range(int(y_lim.min()), int(y_lim.max())):\n","        ax.axhline(i, linestyle='--', color='#ecf0f1', zorder=0)\n","        ax.plot([0.05, -0.05], [i, i], color='#2c3e50')\n","    # x and y axis\n","    ax.axhline(0, color='#2c3e50', zorder=0)\n","    ax.axvline(0, color='#2c3e50', zorder=0)\n","    ax.scatter([0], [0], color='#c0392b', zorder=0)\n","\n","    ax.grid(False)\n","    ax.set_xlim(x_lim)\n","    ax.set_ylim(y_lim)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bIn87rk87cgI"},"source":["## Definition\n","\n","Linear Regression \n","- supervised machine learning algorithm\n","- solves a **regression** problem. \n","- **Input**: **vector** $x \\in R^n$ \n","- **Output**: **scalar** $y \\in R$.\n","- The value that our model predicts y is called $\\hat{y}$, which is defined as:\n","\n","$$\n","\\hat{y} = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b = b + \\sum^n{w_ix_i} = w^Tx + b\n","$$\n","\n","where\n","\n","<div align=\"center\">\n","\n","$w \\in R^n$, and $b \\in R$ are parameters\n","\n","$w$ is the vector of **coefficients**, also known as set of **weights**\n","\n","$b$ is the **intercept**, also known as the **bias**\n","\n","</div>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"MyE4lMoXoj74"},"source":["<div align=\"center\">\n","    <img src=\"https://i.imgur.com/b7zoo7n.png\" width=\"600\" />\n","</div>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DrMlPVR_kxIr"},"source":["Our goal is to find the best fitting line (or hyperplane) that **minimizes mean squared error (MSE)** between our target variable (y) and our predicted output $\\hat y$ over all samples in our dataset\n","\n","$$SSE = \\sum_{i=1}^{n}(y - \\hat y)^2$$\n","$$MSE = \\frac{1}{n}SSE$$\n","\n","This is also known as **Ordinary Least Squares (OLS) Linear Regression**\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ep9u06fFkxIt"},"source":["**That means we need to estimate parameters w and b**\n","\n","We will use an **optimization algorithm** known as **Gradient Descent** (and later Stochastic Gradient Descent or SGD) to solve this problem"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LkmSyAWrkxIu"},"outputs":[],"source":["x = np.array([0, 1, 2, 3, 4, 5], dtype=int) # years of experience\n","y = np.array([2.75, 7.2, 9.4, 16.9, 17.24, 24.32], dtype=float) # salary\n","\n","def line_lr(x):\n","    return w*x + b\n","\n","def mse(y, y_hat):\n","    return ((y_hat - y)**2).mean()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["👉 Senario 1: w = 3.2, b = 5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":375},"executionInfo":{"elapsed":982,"status":"ok","timestamp":1629344234668,"user":{"displayName":"Quan Tran","photoUrl":"","userId":"11078860236930939203"},"user_tz":-420},"id":"-84NIpVnkxIz","outputId":"1768906c-931d-4933-8993-0a7f5ddacbf1"},"outputs":[],"source":["w = 3.2\n","b = 5\n","\n","# -------------------PLOT CHART-------------------------\n","y_hat = line_lr(x) # calculate the prediction\n","line_x = np.arange(-1, 7)\n","\n","plt.figure(figsize=(10, 5))\n","\n","draw_grid(x_lim=np.array([-1, 6]), y_lim=np.array([-1, 25]))\n","\n","plt.scatter(x, y, label='y')\n","plt.scatter(x, y_hat, c='r', label='y^')\n","plt.plot(line_x, line_lr(line_x), c='r')\n","for i, y_i in enumerate(y):\n","    plt.plot([i, i], [y_i, line_lr(i)], linestyle='--', c='g')\n","plt.title(f'Loss function L(w,b) = {mse(y, y_hat)} \\n for w={w} and b={b}, y^={w}x + {b}', fontsize=18)\n","\n","plt.xlabel('Years of experience')\n","plt.ylabel('Salary')\n","plt.legend()\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["👉 Senario 2: w = 4.2, b = 3"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["w = 4\n","b = 3\n","\n","# -------------------PLOT CHART-------------------------\n","y_hat = line_lr(x) # calculate the prediction\n","line_x = np.arange(-1, 7)\n","\n","plt.figure(figsize=(10, 5))\n","\n","draw_grid(x_lim=np.array([-1, 6]), y_lim=np.array([-1, 25]))\n","\n","plt.scatter(x, y, label='y')\n","plt.scatter(x, y_hat, c='r', label='y^')\n","plt.plot(line_x, line_lr(line_x), c='r')\n","for i, y_i in enumerate(y):\n","    plt.plot([i, i], [y_i, line_lr(i)], linestyle='--', c='g')\n","plt.title(f'Loss function L(w,b) = {mse(y, y_hat)} \\n for w={w} and b={b}, y^={w}x + {b}', fontsize=18)\n","\n","plt.xlabel('Years of experience')\n","plt.ylabel('Salary')\n","plt.legend()\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QEhdmftCkxI8"},"source":["# Get data"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"y5AOapefkxI9"},"source":["We are given the dataset with the following columns (features): how much a company spends on Radio, TV and Newspaper advertising each year and its annual Sales in terms of units sold. \n","\n","We are trying to develop an equation that will let us to **predict units sold in thousands (Sales column) based on how much a company spends on advertising (US dollar, in thousand)**. The rows represent companies."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJfnwr5ykxI-"},"outputs":[],"source":["df = pd.read_csv('data\\\\advertising.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":205},"executionInfo":{"elapsed":457,"status":"ok","timestamp":1629345131890,"user":{"displayName":"Quan Tran","photoUrl":"","userId":"11078860236930939203"},"user_tz":-420},"id":"MdUkjugJkxJJ","outputId":"237e0ef0-5674-4b08-bb62-2b137d8e5cf8"},"outputs":[],"source":["df.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"djgcZSOUkxJT"},"source":["Using a real dataset above, we will estimate of sales given a company's TV  advertising spent\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3bgU2KEakxJU"},"outputs":[],"source":["X = df[['TV']]\n","y = df[['Sales']]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize = (8,5))\n","sns.scatterplot(x = X['TV'], y = y['Sales'])\n","plt.title('TV vs SALES')\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rQM8qTMKkxJi"},"source":["$$Sales = TV*Weight + Bias$$\n","$$y = Xw + b$$\n","- **TV is our independent variable**. In machine learning we also call these variables **features**.\n","- **Sales is dependent variable**. This is what we have to predict\n","- **Weight is the coefficient** for the TV independent variable. In machine learning we call coefficients weights.\n","- **Bias is the intercept where our line intercepts the y-axis**. In machine learning we can call intercepts bias. Bias offsets all predictions that we make.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"efhuMhWvkxJk"},"source":["We will try to **learn the correct values for Weight and Bias**. By the end of our training, our equation will approximate the line of best fit."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Single Feature Linear Regression"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Train Test Split:\n","\n","<img src=\"https://cdn-coiao.nitrocdn.com/CYHudqJZsSxQpAPzLkHFOkuzFKDpEHGF/assets/static/optimized/rev-85bf93c/wp-content/uploads/2022/05/sklearn-train-test-split_syntax-explanation_v2.png\" width=\"600\">"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import train test split function:\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n","\n","print('X train shape', X_train.shape)\n","print('y train shape', y_train.shape)\n","print('X test shape', X_test.shape)\n","print('y test shape', y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize = (8,5))\n","sns.scatterplot(x = X_train['TV'], y = y_train['Sales'], label = 'Train data')\n","sns.scatterplot(x = X_test['TV'], y = y_test['Sales'], color = 'red', label = 'Test data')\n","plt.title('TV vs SALES ')\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Import and Train model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZZ5KZ73kxJl"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, mean_absolute_error"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Call model:\n","lr1 = LinearRegression()\n","lr1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fit Train set (Train model)\n","lr1.fit(X_train, y_train)\n","lr1 #<= trained model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Lấy weight và bias:\n","w = lr1.coef_\n","b = lr1.intercept_\n","\n","print('weight', w)\n","print('bias', b)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":319},"executionInfo":{"elapsed":1029,"status":"ok","timestamp":1629345523294,"user":{"displayName":"Quan Tran","photoUrl":"","userId":"11078860236930939203"},"user_tz":-420},"id":"DFVwyQjBkxJp","outputId":"e2edbb4d-e8e2-4335-cfd9-f91b238942d8"},"outputs":[],"source":["w = lr1.coef_\n","b = lr1.intercept_\n","\n","print(f'Weight: {w}')\n","print(f'Bias: {b}')\n","print(f'MSE: {mean_squared_error(y_train, y_train_pred)}')\n","\n","plt.figure(figsize = (8,5))\n","sns.scatterplot(x = X_train['TV'], y = y_train['Sales'])\n","sns.scatterplot(x = X_test['TV'], y = y_test['Sales'], color = 'red')\n","\n","plt.plot(np.array(X), lr1.predict(X))\n","plt.title(f'y = {w[0][0]}x + {b[0]}', fontsize = 15)\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"y2ZdhKqJkxJv"},"source":["## Loss function"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"L7eP1uL5kxJw"},"source":["As mentioned above, we will use Mean Square Error as our loss function\n","\n","A loss function is a wrapper around our model function that tells us \"how good\" our model is at making predictions for a given set of parameters. The loss function has its own curve and its own derivatives. The slope of this curve tells us the direction we should update our weights to make the model more accurate!"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4OSwGMoakxJx"},"source":["For our simple linear equation:\n","\n","y = wx + b\n","\n","MSE can be calculated with the formula:\n","\n","$$MSE =  \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (wx_i + b))^2$$\n","- $N$ is the total number of observations (data points)\n","- $\\frac{1}{N} \\sum_{i=1}^{n}$ is the mean\n","- $y_i$ is the actual value of an observation and $(mx_i + b)$ is our prediction"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["👉 Here are three common evaluation metrics for regression problems:\n","\n","**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n","\n","$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n","\n","**Mean Squared Error** (MSE) is the mean of the squared errors:\n","\n","$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n","\n","**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n","\n","$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n","\n","Comparing these metrics:\n","\n","- **MAE** is the easiest to understand, because it's the average error.\n","- **MSE** is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n","- **RMSE** is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n","\n","All of these are **loss functions**, because we want to minimize them."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# predict on train set:\n","y_train_pred = lr1.predict(X_train)\n","\n","# Loss functions on train data:\n","print('>>> On Training data: <<<')\n","print('MSE:', mean_squared_error(y_train, y_train_pred))\n","print('MAE:', mean_absolute_error(y_train, y_train_pred))\n","print('RMSE:', mean_squared_error(y_train, y_train_pred)**0.5)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make prediction on testset:\n","y_test_prd = lr1.predict(X_test)\n","\n","# Loss functions on test data:\n","print('>>> On Testing data <<<')\n","print('MSE:', mean_squared_error(y_test, y_test_prd))\n","print('MAE:', mean_absolute_error(y_test, y_test_prd))\n","print('RMSE:', mean_squared_error(y_test, y_test_prd)**0.5)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make MSE colections:\n","mse_dct = {}\n","mse_dct['TV'] = [mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_prd)]\n","mse_dct"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##### 🤔 Predict Sales when the company pay for TV adv 200 USD!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# First let's see the shape of X:\n","X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Your input must be a 2-d array:\n","ip = np.array([[200]])\n","ip.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lr1.predict(ip)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["👉 when the company pay 200 USD for TV adv. they will get 16.45 USD in sales"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Your turn: predict company sales when they pay 300 USD on TV adv.\n","ip2 = np.array([[300]])\n","lr1.predict(ip2)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"A9kdr7GMVHs4"},"source":["# Multiple linear regression"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"q5CeSCNRkxJ7"},"source":["Now we will consider all the features for our sale prediction"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":309,"status":"ok","timestamp":1629356926992,"user":{"displayName":"Quan Tran","photoUrl":"","userId":"11078860236930939203"},"user_tz":-420},"id":"QnQX6-KpVOfm","outputId":"b3a50d5c-7a6d-4192-ad21-a764a957017e"},"outputs":[],"source":["# Features: TV, Radio, Newspaper\n","X = df[['TV', 'Radio', 'Newspaper']]\n","y = df[['Sales']]\n","X.shape, y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8152,"status":"ok","timestamp":1620718647674,"user":{"displayName":"Chinh Nguyen","photoUrl":"","userId":"07486780944576475147"},"user_tz":-420},"id":"VyLbjzpMZt8C","outputId":"1faab405-8027-493a-8c4f-be7b86fb86de"},"outputs":[],"source":["# Train test split:\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n","\n","print('X train shape', X_train.shape)\n","print('y train shape', y_train.shape)\n","print('X test shape', X_test.shape)\n","print('y test shape', y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8046,"status":"ok","timestamp":1620718647675,"user":{"displayName":"Chinh Nguyen","photoUrl":"","userId":"07486780944576475147"},"user_tz":-420},"id":"50xQ6q-BZ9JA","outputId":"6fccab52-cb79-4aaf-ef92-020615b4d762"},"outputs":[],"source":["# Train model (fit model):\n","lr2 = LinearRegression()\n","lr2.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Weights and bias:\n","print(f'Weights: {lr2.coef_}')\n","print(f'Bias: {lr2.intercept_}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Prediction on Trainset:\n","y_train_pred_2 = lr2.predict(X_train)\n","y_train_pred_2[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# predict on testset:\n","y_test_prd_2 = lr2.predict(X_test)\n","y_test_prd_2[:5]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Evaluation:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loss functions on train data:\n","print('>>> On Training data: <<<')\n","print('MSE:', mean_squared_error(y_train, y_train_pred_2))\n","print('MAE:', mean_absolute_error(y_train, y_train_pred_2))\n","print('RMSE:', mean_squared_error(y_train, y_train_pred_2)**0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predict on Test-set:\n","y_test_pred_2 = lr2.predict(X_test)\n","# Loss functions on test data:\n","print('>>> On Testing data <<<')\n","print('MSE:', mean_squared_error(y_test, y_test_pred_2))\n","print('MAE:', mean_absolute_error(y_test, y_test_pred_2))\n","print('RMSE:', mean_squared_error(y_test, y_test_pred_2)**0.5)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# MSE collection:\n","mse_dct['TV, Radio, Newspaper'] = [mean_squared_error(y_train, y_train_pred_2), mean_squared_error(y_test, y_test_pred_2)]\n","\n","# Make plot dataframe:\n","dfplot = pd.DataFrame(mse_dct).T.reset_index()\n","dfplot.columns = ['case', 'Train - MSE', 'Test - MSE']\n","dfplot = pd.melt(dfplot, id_vars= 'case', value_vars= ['Train - MSE', 'Test - MSE'])\n","\n","# Plot:\n","plt.figure(figsize=(10, 5))\n","ax = sns.barplot(data = dfplot, x = 'case', y = 'value', hue = 'variable')\n","plt.title('MSE COMPARISON')\n","for i in range(dfplot.variable.nunique()):\n","    plt.bar_label(ax.containers[i])\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Transformer - MinMaxScaler:\n","\n","Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n","\n","Transform features by scaling each feature to a given range."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 🤔 Example:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example data:\n","data = np.array([[-1, 2], [-0.8, 6], [0, 10], [1, 18]])\n","print(data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Scale example data to 0-1:\n","scaler = MinMaxScaler((0, 1))\n","scaler.fit(data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(scaler.transform(data))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 👉 Apply to Advertise data:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Khai báo scaler:\n","scaler = MinMaxScaler()\n","scaler"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fit on trainset:\n","scaler.fit(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Trainsform data on trainset and testset:\n","X_train_scaler = scaler.transform(X_train)\n","X_test_scaler = scaler.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train_scaler[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(12, 4))\n","\n","plt.subplot(121)\n","sns.histplot(X_train['TV'])\n","plt.title('Before using MinMaxScaler')\n","\n","plt.subplot(122)\n","sns.histplot(X_train_scaler[:, 0])\n","plt.title('After using MinMaxScaler')\n","plt.xlabel('TV')\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Train model:**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train model (fit model):\n","lr3 = LinearRegression()\n","lr3.fit(X_train_scaler, y_train)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Prediction on Trainset:\n","y_train_pred_3 = lr3.predict(X_train_scaler)\n","print(f'Train-set MSE: {mean_squared_error(y_train, y_train_pred_3)}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predict on Test-set:\n","y_test_pred_3 = lr3.predict(X_test_scaler)\n","print(f'Test-set MSE: {mean_squared_error(y_test, y_test_pred_3)}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# MSE collection:\n","mse_dct['TV, Radio, Newspaper \\n+ MinMaxScaler'] = [mean_squared_error(y_train, y_train_pred_3),\\\n","                                                   mean_squared_error(y_test, y_test_pred_3)]\n","\n","# Make plot dataframe:\n","dfplot = pd.DataFrame(mse_dct).T.reset_index()\n","dfplot.columns = ['case', 'Train - MSE', 'Test - MSE']\n","dfplot = pd.melt(dfplot, id_vars= 'case', value_vars= ['Train - MSE', 'Test - MSE'])\n","\n","# Plot:\n","plt.figure(figsize=(10, 5))\n","ax = sns.barplot(data = dfplot, x = 'case', y = 'value', hue = 'variable')\n","plt.title('MSE COMPARISON')\n","for i in range(dfplot.variable.nunique()):\n","    plt.bar_label(ax.containers[i])\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"jfRHcRtrkHst"},"source":["## Polynomial Linear Regression & Overfitting/Underfitting\n","\n","<img src=\"https://static.javatpoint.com/tutorial/machine-learning/images/machine-learning-polynomial-regression.png\" width=\"600\">"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Xmr6YYZgkaLg"},"source":["A polynomial degree 1 gives us the simple linear regression model:\n","\n","$$\n","\\hat{y} = wx + b,\\ with\\ x, \\hat{y} \\in R\n","$$"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"89625DI6kxCG"},"source":["By adding $x^2$ as another feature, the model becomes a quadratic function of $x$:\n","\n","$$\n","\\hat{y} = w_1x + w_2x^2 + b\n","$$"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2uMuWTSalRd7"},"source":["We can continue to add more powers of $x$ as additional features to obtain a polynomial of degree $n$\n","\n","$$\n","\\hat{y} = b + \\sum_{i=1}^n{w_ix^i}\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PzVi2rXlTFYC"},"outputs":[],"source":["df = pd.read_csv('data\\\\poly_example_1.csv')\n","x = df[['X']].values\n","y = df[['Y']].values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RP-0jTDO6as9"},"outputs":[],"source":["# import PolynomialFeatures\n","from sklearn.preprocessing import PolynomialFeatures"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xPb0996f6QmG"},"outputs":[],"source":["# Define a instance of PolynomialFeatures with degree=2 called 'poly_reg'\n","poly_reg = PolynomialFeatures(degree = 3, include_bias= False)\n","\n","# Fit and transform 'X_train', 'X_test' with the PolynomialFeatures and save the result in 'X_train_poly' and 'X_test_poly'\n","X_poly = poly_reg.fit_transform(x) \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1635314576610,"user":{"displayName":"Quan Tran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11078860236930939203"},"user_tz":-420},"id":"XmkenF-lVXUc","outputId":"ea3a3999-3dd4-4121-f47f-923f17296738"},"outputs":[],"source":["x[:5], X_poly[:5]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RpGgoKv4lv27"},"source":["#### **Example**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":323},"executionInfo":{"elapsed":1304,"status":"ok","timestamp":1635313682959,"user":{"displayName":"Quan Tran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11078860236930939203"},"user_tz":-420},"id":"WVcq-rJrmCCT","outputId":"7407d419-e638-4be4-b7cd-90e0596e6519"},"outputs":[],"source":["x = df[['X']].values\n","y = df[['Y']].values\n","\n","# Let's plot the dataset using plt.scatter()\n","plt.figure(figsize=(10, 5))\n","\n","# Your code here\n","plt.scatter(x, y)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f14Flg-LmSHd"},"outputs":[],"source":["# Split the data in 80% training and 20% validation\n","from sklearn.model_selection import train_test_split\n","\n","X_train1, X_test1, y_train1, y_test1 = train_test_split(x, y, test_size=0.2, random_state=102)\n","\n","print('X train shape', X_train1.shape)\n","print('y train shape', y_train1.shape)\n","print('X test shape', X_test1.shape)\n","print('y test shape', y_test1.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":323},"executionInfo":{"elapsed":662,"status":"ok","timestamp":1635313897461,"user":{"displayName":"Quan Tran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11078860236930939203"},"user_tz":-420},"id":"GlEqcZDInABp","outputId":"9aa8c2ba-9589-4a5f-dd00-d26e164dcc61"},"outputs":[],"source":["plt.figure(figsize=(10, 5))\n","plt.scatter(X_train1, y_train1)\n","plt.scatter(X_test1, y_test1)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"executionInfo":{"elapsed":1248,"status":"ok","timestamp":1635314590146,"user":{"displayName":"Quan Tran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11078860236930939203"},"user_tz":-420},"id":"ozMOHaSInG9M","outputId":"fab0bcee-004a-4988-da76-0bbdd69dcc0f"},"outputs":[],"source":["\n","degree = 20 # hyperparameter\n","\n","# Define a instance of LinearRegression called 'lr'\n","lr = LinearRegression()\n","\n","# Define a instance of PolynomialFeatures with degree called 'poly_reg'\n","poly_reg = PolynomialFeatures(degree = degree) \n","\n","# Fit and transform 'X_train1', 'X_test1' with the PolynomialFeatures and save the result in 'X_train1_poly' and 'X_test1_poly'\n","X_train1_poly = poly_reg.fit_transform(X_train1)\n","X_test1_poly = poly_reg.fit_transform(X_test1)\n","\n","# Fitting the Linear Regression Model to the training set (X_train1_poly)\n","lr.fit(X_train1_poly, y_train1)\n","\n","# Predict test set and save the result in 'y_test1_predict'\n","y_test1_predict = lr.predict(X_test1_poly)\n","y_train1_predict = lr.predict(X_train1_poly)\n","\n","print('degree:', degree)\n","print(f'MSE on train set: {mean_squared_error(y_train1,y_train1_predict)}')\n","print(f'MSE on validation set: {mean_squared_error(y_test1,y_test1_predict)}')\n","\n","\n","# plotting purposes\n","x_plot = np.linspace(x.min(), x.max(), 1000).reshape(-1, 1)\n","y_plot = lr.predict(poly_reg.fit_transform(x_plot))\n","plt.figure(figsize=(10, 5))\n","plt.plot(x_plot, y_plot)\n","plt.scatter(X_train1, y_train1)\n","plt.scatter(X_test1, y_test1)\n","plt.xlim(-6, 6)\n","plt.ylim(-40, 40)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Lặp qua các degree và tính loss function trên trainset và testset:\n","results = {'degree': [], 'train_mse': [], 'test_mse': []}\n","\n","for d in range(1, 22):\n","\n","    degree = d\n","\n","    # Define a instance of LinearRegression called 'lr'\n","    lr = LinearRegression()\n","\n","    # Define a instance of PolynomialFeatures with degree called 'poly_reg'\n","    poly_reg = PolynomialFeatures(degree = degree) \n","\n","    # Fit and transform 'X_train1', 'X_test1' with the PolynomialFeatures and save the result in 'X_train1_poly' and 'X_test1_poly'\n","    X_train1_poly = poly_reg.fit_transform(X_train1)\n","    X_test1_poly = poly_reg.fit_transform(X_test1)\n","\n","    # Fitting the Linear Regression Model to the training set (X_train1_poly)\n","    lr.fit(X_train1_poly, y_train1)\n","\n","    # Predict test set and save the result in 'y_test1_predict'\n","    y_test1_predict = lr.predict(X_test1_poly)\n","    y_train1_predict = lr.predict(X_train1_poly)\n","\n","    # Save to result dict:\n","    results['degree'].append(d)\n","    results['train_mse'].append(mean_squared_error(y_train1,y_train1_predict))\n","    results['test_mse'].append(mean_squared_error(y_test1,y_test1_predict))\n","\n","result_df = pd.DataFrame(results)\n","result_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = plt.subplots(figsize =(12, 6))\n"," \n","# Make the plot\n","plt.plot(result_df['degree'], result_df['train_mse'],label ='Train MSE')\n","plt.plot(result_df['degree'], result_df['test_mse'], label ='Test MSE')\n"," \n","# Adding Xticks\n","plt.xlabel('Polynomial Degree')\n","plt.ylabel('Loss')\n","\n","plt.xticks(result_df['degree'])\n"," \n","plt.legend()\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"EKkxBzwrkxLr"},"source":["![](https://www.researchgate.net/profile/Hayder_Al-Behadili/publication/325999203/figure/fig4/AS:641844216074241@1530038994324/Overfitting-and-underfitting-effect-on-error.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 👉 Apply to Advertise data "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Lặp qua các degree và tính loss function trên trainset và testset khi áp dụng polynomialFeature\n","results = {'degree': [], 'train_mse': [], 'test_mse': []}\n","\n","for d in range(1, 7):\n","\n","    degree = d\n","\n","    # Define a instance of LinearRegression called 'lr'\n","    lr = LinearRegression()\n","\n","    # Define a instance of PolynomialFeatures with degree called 'poly_reg'\n","    poly_reg = PolynomialFeatures(degree = degree) \n","\n","    # Fit and transform 'X_train_scaler', 'X_test_scaler' with the PolynomialFeatures and save the result in 'X_train_poly' and 'X_test_poly'\n","    X_train_poly = poly_reg.fit_transform(X_train_scaler)\n","    X_test_poly = poly_reg.fit_transform(X_test_scaler)\n","\n","    # Fitting the Linear Regression Model to the training set (X_train_poly)\n","    lr.fit(X_train_poly, y_train)\n","\n","    # Predict test set and save the result in 'y_test_predict'\n","    y_test_predict = lr.predict(X_test_poly)\n","    y_train_predict = lr.predict(X_train_poly)\n","\n","    # Save to result dict:\n","    results['degree'].append(d)\n","    results['train_mse'].append(mean_squared_error(y_train,y_train_predict))\n","    results['test_mse'].append(mean_squared_error(y_test,y_test_predict))\n","\n","result_df = pd.DataFrame(results)\n","result_df.head()\n","\n","# Plot MSE \n","fig = plt.subplots(figsize =(12, 5))\n","# Make the plot\n","plt.plot(result_df['degree'], result_df['train_mse'],label ='Train MSE')\n","plt.plot(result_df['degree'], result_df['test_mse'], label ='Test MSE')\n"," \n","# Adding Xticks\n","plt.xlabel('Polynomial Degree')\n","plt.ylabel('Loss')\n","\n","plt.xticks(result_df['degree'])\n"," \n","plt.legend()\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["✳️ with degree = 5 give us the best model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Call Linear Regression model:\n","lr4 = LinearRegression()\n","\n","# Call PolynomialFeatures :\n","poly_reg = PolynomialFeatures(degree = 5)\n","\n","# Fit and transform 'X_train_scaler', 'X_test_scaler' with the PolynomialFeatures and save the result in 'X_train_poly' and 'X_test_poly'\n","X_train_poly = poly_reg.fit_transform(X_train_scaler)\n","X_test_poly = poly_reg.fit_transform(X_test_scaler)\n","\n","# Fitting (Training model) the Linear Regression Model to the training set (X_train_poly)\n","lr4.fit(X_train_poly, y_train)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Prediction on Trainset:\n","y_train_pred_4 = lr4.predict(X_train_poly)\n","print(f'Train-set MSE: {mean_squared_error(y_train, y_train_pred_4)}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predict on Test-set:\n","y_test_pred_4 = lr4.predict(X_test_poly)\n","print(f'Test-set MSE: {mean_squared_error(y_test, y_test_pred_4)}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# MSE collection:\n","mse_dct['TV, Radio, Newspaper \\n+ MinMaxScaler \\n+ PolynomialFeatures'] = [mean_squared_error(y_train, y_train_pred_4),\\\n","                                                   mean_squared_error(y_test, y_test_pred_4)]\n","\n","# Make plot dataframe:\n","dfplot = pd.DataFrame(mse_dct).T.reset_index()\n","dfplot.columns = ['case', 'Train - MSE', 'Test - MSE']\n","dfplot = pd.melt(dfplot, id_vars= 'case', value_vars= ['Train - MSE', 'Test - MSE'])\n","\n","# Plot:\n","plt.figure(figsize=(10, 5))\n","ax = sns.barplot(data = dfplot, x = 'case', y = 'value', hue = 'variable')\n","plt.title('MSE COMPARISON')\n","for i in range(dfplot.variable.nunique()):\n","    plt.bar_label(ax.containers[i])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train_scaler.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["🤔 **Predict sales with paying adv. for TV: 200; Radio: 20; Newspaper: 30**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predict Sales when company pay\n","#step1: create input:\n","ip_arr = np.array([[200, 20, 30]])\n","\n","#step2: using MinMaxScaler:\n","ip_scl = scaler.transform(ip_arr)\n","\n","#step3: using polynomial:\n","ip_scl_poly = poly_reg.transform(ip_scl)\n","\n","#step4: predict!\n","lr4.predict(ip_scl_poly)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# GEATE JOB! 😉"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"5.4a_LinearRegression.ipynb","provenance":[]},"kernelspec":{"display_name":"DA","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"f90745bbca20a4cb7db8559fdb79554de2a76cb1e5146bbf788699cf2d961a52"}}},"nbformat":4,"nbformat_minor":0}
