{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Python\n",
    "\n",
    "For this lecture we will be working with the [Titanic Data Set from Kaggle](https://www.kaggle.com/c/titanic). This is a very famous data set and very often is a student's first step in machine learning! \n",
    "\n",
    "We'll be trying to predict a classification- survival or deceased.\n",
    "Let's begin our understanding of implementing Logistic Regression in Python for classification.\n",
    "\n",
    "We'll use a \"semi-cleaned\" version of the titanic data set, if you use the data set hosted directly on Kaggle, you may need to do some additional cleaning not shown in this lecture notebook.\n",
    "\n",
    "## Import Libraries\n",
    "Let's import some libraries to get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Ignore warning\n",
    "\n",
    "pd.set_option('float_format', '{:2f}'.format) # Show full number instead of show number like \"1.5e2\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Let's start by reading in the titanic_train.csv file into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data\\\\titanic_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4))\n",
    "sns.boxplot(x='Pclass', y='Age', data=df, palette='winter')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the wealthier passengers in the higher classes tend to be older, which makes sense. We'll use these median age values to impute based on Pclass for Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean of age each pclass:\n",
    "median1 = df[df['Pclass'] == 1]['Age'].median()\n",
    "median2 = df[df['Pclass'] == 2]['Age'].median()\n",
    "median3 = df[df['Pclass'] == 3]['Age'].median()\n",
    "\n",
    "median1, median2, median3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute age to each class:\n",
    "df.loc[(df['Pclass'] == 1) & (df['Age'].isna()), 'Age'] = median1\n",
    "df.loc[(df['Pclass'] == 2) & (df['Age'].isna()), 'Age'] = median2\n",
    "df.loc[(df['Pclass'] == 3) & (df['Age'].isna()), 'Age'] = median3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cabin field"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's go ahead and drop the Cabin column and the row in Embarked that is NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.drop(columns='Cabin', inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embarked field"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to drop the Embarked field because there were only two null values and you couldn’t use other fields to predict the missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop remaining NA values:\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 Field 'sex': assigning male: 1; female: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Sex'] == 'male', 'Sex'] = 1\n",
    "df.loc[df['Sex'] == 'female', 'Sex'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 Field 'Embarked': using 'get_dummy' (or OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pd.get_dummies()\n",
    "embark = pd.get_dummies(df['Embarked'])\n",
    "embark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ordinal encoder to \n",
    "oh_encoder = OneHotEncoder(sparse_output=False)\n",
    "embarked_enc = oh_encoder.fit_transform(df[['Embarked']])\n",
    "embarked_enc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show onehot categories:\n",
    "oh_encoder.categories_[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 Combine with transformed data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot = pd.DataFrame(embarked_enc, columns= oh_encoder.categories_[0])\n",
    "df_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df, df_onehot], axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 Drop unnecessary columns: `PassengerId`, `Name`, `Ticket`, `Embarked`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns:\n",
    "df2.drop(columns=['PassengerId','Embarked','Name','Ticket'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "😉 Great! Our data is ready for our model!\n",
    "\n",
    "# Building a Logistic Regression model\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.saedsayad.com/images/LogReg_1.png\" width=\"600\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Similar to linear regression, `logistic regression` is also used to` estimate the relationship between a dependent variable` and `one or more independent variables`, but it is used to make a prediction about a `categorical variable` versus a continuous one. A categorical variable can be true or false, yes or no, 1 or 0, et cetera. The unit of measure also differs from linear regression as `it produces a probability`, but the logit function transforms the S-curve into straight line.\n",
    "\n",
    "+ As default, Sklearn Logistic Regression uses 0.5 as the threshold to classify 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df2.drop(columns='Survived'), \n",
    "                                                    df2['Survived'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=101)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg1 = LogisticRegression()\n",
    "logreg1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make prediction and return result as label:\n",
    "y_train_pred = logreg1.predict(X_train)\n",
    "y_test_pred = logreg1.predict(X_test)\n",
    "\n",
    "# Make prediction and return result as probability:\n",
    "y_train_pred_prop = logreg1.predict_proba(X_train)\n",
    "y_test_pred_prop = logreg1.predict_proba(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look in our results:\n",
    "print('Result of \".predict(X_train)\":', y_train_pred[:5], sep = '\\n')\n",
    "print('=='*30)\n",
    "print('Result of \".predict_proba(X_train)\":', y_train_pred_prop[:5], sep = '\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation metrics:\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 👉 Accuracy metrics\n",
    "The ratio between the number of correctly predicted points and the total number of points in the data set. \n",
    "\n",
    "It's simple! Right :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on trainset:\n",
    "accuracy_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on testset:\n",
    "accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 👉 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating non-normalized confustion matrix on Testset:\n",
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized confustion matrix on Testset\n",
    "confusion_matrix(y_test, y_test_pred, normalize='true')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (11, 4))\n",
    "plt.subplot(121)\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "df_cm = pd.DataFrame(conf_matrix, columns=np.unique(['Negative (0)', 'Positive (1)']), \n",
    "                     index = np.unique(['Negative (0)', 'Positive (1)']))\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "sns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})\n",
    "\n",
    "plt.subplot(122)\n",
    "conf_matrix_norm = confusion_matrix(y_test, y_test_pred, normalize='true')\n",
    "df_cm = pd.DataFrame(conf_matrix_norm, columns=np.unique(['Negative (0)', 'Positive (1)']), \n",
    "                     index = np.unique(['Negative (0)', 'Positive (1)']))\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "sns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 14})\n",
    "plt.suptitle('CONFUSION MATRIX')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_test_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 **Type 1 and Type 2 errors**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3NycrFcQRWBY"
   },
   "source": [
    "![alt text](https://www.statisticssolutions.com/wp-content/uploads/2017/12/rachnovblog.jpg)\n",
    "\n",
    "source: https://www.statisticssolutions.com/to-err-is-human-what-are-type-i-and-ii-errors/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9ssxfPvg5Qh5"
   },
   "source": [
    "+ **Type 1 error** (**False Positive**), Xảy ra khi giả thuyết **thực chất là sai** nhưng **được cho là đúng**\n",
    "\n",
    "    *+ Ví dụ:* Bạn xây dựng model để dự đoán bệnh nhân có bị covid hay không (Trong đó, positive là khỏe mạnh và negative là bị covid). \n",
    "    \n",
    "    Nếu model dự đoán bệnh nhân khỏe mạnh nhưng thực tế họ có bị thì đó được gọi là một Type 1 error\n",
    "\n",
    "+ **Type 2 error** (**False Negative**), xảy ra khi giả thuyết **thực chất là đúng** nhưng **được cho là sai**\n",
    "\n",
    "    *+ Ví dụ:* Bạn xây dựng model để dự đoán bệnh nhân có bị covid hay không (Trong đó, positive là khỏe mạnh và negative là bị covid). \n",
    "    \n",
    "    Nếu model dự đoán bệnh nhân bị covid nhưng thực tế họ lại không bị thì đó được gọi là một Type 2 error*\n",
    "\n",
    "Thông thường chúng ta cần xem xét việc giảm cả 2 loại lỗi này để model của chúng ta đạt hiệu quả cao nhất\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tgZXvm79MzlY"
   },
   "source": [
    "### Terminologies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "k5XVughUMb0O"
   },
   "source": [
    "**Recall, sensitivity, hit rate, or true positive rate(TPR)**: là tỉ lệ số điểm true positive trong số những điểm thực sự là positive (TP + FN). Hay nói cách khác là tỉ lệ model dự đoán đúng là Positive (1) trên tổng số thực tế Positive (1) của data\n",
    "\n",
    "Để tăng Recall ta cần giảm FN, tức là giảm Type 2 error\n",
    "\n",
    "\n",
    "$$\n",
    "Recall = \\frac{TP}{P} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "**Precision or positive predictive value (PPV)**: là tỉ lệ số điểm true positive trong số những điểm được phân loại là positive của model (TP + FP). Hay nói cách khác là tỉ lệ model dự đoán đúng là Positive (1) trên tổng số dự đoán là Positive (1) của model \n",
    "\n",
    "Để tăng Precision ta cần giảm FP, tức là giảm Type 1 error\n",
    "\n",
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "\n",
    "**F1 score**: the **harmonic mean** of **precision** and **recall**\n",
    "\n",
    "$$\n",
    "F_1 = 2 \\frac{Precision . Recall}{Precision + Recall}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check `precision`, `recall`, `f1-score` using `classification report`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_test_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤔 Note: If you curious about `macro avg` and `weight avg` and here is the answer:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ In the case of **Weighted average** the performance metrics are weighted accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Weight Avg. of recall (The same with Precision):\n",
    "# (Percentage_of_positive)*Positive_Recall + (Percentage_of_Negative)*Negative_Recall\n",
    "\n",
    "(71/178)*0.66 + (107/178)*0.92"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ In the case of **Macro average** is just the **mean** of metrics of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Macro Avg. of recall (The same with Precision):\n",
    "# (Positive_Recall + Negative_Recall)/2\n",
    "\n",
    "(0.66 + 0.92)/2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤔 Let's try to train model without `Parch` column and Use SMOTE OverSampling technique "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 Drop Parch column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.drop(columns = 'Parch')\n",
    "df3.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👉 Use SMOTE OVERSAMPLING technique\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/347937180/figure/fig3/AS:973429209563136@1609095017080/Illustration-of-the-SMOTE-oversampling-approach.ppm\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df3.drop(columns='Survived')\n",
    "y = df3['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install imblearn library\n",
    "! pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# X và y là các feature và label của dữ liệu\n",
    "smote = SMOTE(k_neighbors = 3, random_state=96)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled.shape, y_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check value count of label:\n",
    "y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_resampled, \n",
    "                                                    y_resampled, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=101,\n",
    "                                                    stratify = y_resampled)\n",
    "\n",
    "X_train1.shape, y_train1.shape, X_test1.shape, y_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model:\n",
    "logreg2 = LogisticRegression()\n",
    "# Train model:\n",
    "logreg2.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on trainset and testset:\n",
    "y_test_pred1 = logreg2.predict(X_test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "# Plot before drop parch col:\n",
    "plt.subplot(221)\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "df_cm = pd.DataFrame(conf_matrix, columns=np.unique(['Negative (0)', 'Positive (1)']), \n",
    "                     index = np.unique(['Negative (0)', 'Positive (1)']))\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "sns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16}, cbar = False)\n",
    "plt.title('CONFUSION MATRIX (before drop PARCH)')\n",
    "\n",
    "plt.subplot(222)\n",
    "conf_matrix_norm = confusion_matrix(y_test, y_test_pred, normalize='true')\n",
    "df_cm = pd.DataFrame(conf_matrix_norm, columns=np.unique(['Negative (0)', 'Positive (1)']), \n",
    "                     index = np.unique(['Negative (0)', 'Positive (1)']))\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "sns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 14}, cbar = False)\n",
    "\n",
    "# Plot after drop parch cols:\n",
    "plt.subplot(223)\n",
    "conf_matrix = confusion_matrix(y_test1, y_test_pred1)\n",
    "df_cm = pd.DataFrame(conf_matrix, columns=np.unique(['Negative (0)', 'Positive (1)']), \n",
    "                     index = np.unique(['Negative (0)', 'Positive (1)']))\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "sns.heatmap(df_cm, cmap=\"Greens\", annot=True,annot_kws={\"size\": 16}, cbar = False)\n",
    "plt.title('CONFUSION MATRIX (after drop PARCH)')\n",
    "\n",
    "plt.subplot(224)\n",
    "conf_matrix_norm = confusion_matrix(y_test1, y_test_pred1, normalize='true')\n",
    "df_cm = pd.DataFrame(conf_matrix_norm, columns=np.unique(['Negative (0)', 'Positive (1)']), \n",
    "                     index = np.unique(['Negative (0)', 'Positive (1)']))\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "sns.heatmap(df_cm, cmap=\"Greens\", annot=True,annot_kws={\"size\": 14}, cbar = False)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification after drop \"Parch\" and Over sampling')\n",
    "print(classification_report(y_test1,y_test_pred1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification before drop \"Parch\" and Over sampling')\n",
    "print(classification_report(y_test,y_test_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's predict\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A customer has data like this:\n",
    "+ Pclass: 2\n",
    "+ Sex: 1\n",
    "+ Age: 25\n",
    "+ SibSp: 0\n",
    "+ Parch: 1\n",
    "+ Fare: 70\n",
    "+ Embarked: Q\n",
    "\n",
    "Will be alive or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f90745bbca20a4cb7db8559fdb79554de2a76cb1e5146bbf788699cf2d961a52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
